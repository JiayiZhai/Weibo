# WeiboSpider - 微博关键词搜索爬虫

这是一个基于Python的微博搜索爬虫，可以根据关键词搜索微博内容。支持批量搜索多个关键词，并将结果保存为CSV文件。支持下载微博中的图片和视频，并提供机器学习模型对内容进行智能分析。

## 功能特点

- 🔍 支持基于多关键词的微博搜索
- 📊 支持将搜索结果导出为CSV文件
- 📷 支持下载微博中的图片
- 🎬 支持下载微博中的视频
- 🧠 提供机器学习分析，筛选高质量内容
- 📈 识别热门话题和趋势
- 🤖 使用BERT和XGBoost模型进行智能分析
- 🛠️ 提供关键词管理工具，方便添加、删除和管理关键词
- 🔄 支持从文本文件导入关键词列表
- 📋 可导出关键词列表为Excel文件
- 🕒 记录微博内容、发布时间、转发量、评论数等信息

## 安装依赖

使用以下命令安装所需依赖：

```bash
pip install -r requirements.txt
```

注意：机器学习功能需要安装额外的依赖，包括torch、transformers、scikit-learn等。这些已包含在requirements.txt中。

## 使用方法

### 1. 准备关键词

你可以通过以下几种方式准备关键词：

- 直接编辑 `keywords.txt` 文件，每行一个关键词
- 使用关键词管理工具添加关键词：`python add_keywords.py`
- 在运行爬虫时，如果没有找到关键词文件，将使用默认的示例关键词

### 2. 运行爬虫

```bash
python main.py
```

程序会提示你输入以下信息：
- (可选) 微博Cookie：提供Cookie可以提高爬取成功率
- 每个关键词要爬取的页数：默认为5页
- 是否下载图片和视频：选择"y"将下载微博中的图片和视频
- 是否使用机器学习分析：选择"y"将使用机器学习模型分析微博内容

### 3. 查看结果

爬取结果将保存在 `results` 目录下：
- 每个关键词的结果会单独保存为一个CSV文件 (`关键词_时间戳.csv`)
- 所有结果会合并保存到一个名为 `all_results_时间戳.csv` 的文件中
- 机器学习分析后的高质量内容会保存为 `关键词_filtered_时间戳.csv`
- 所有过滤后的高质量内容会合并保存到 `all_filtered_results_时间戳.csv`
- 分析结果会保存为JSON格式 (`关键词_analysis_时间戳.json`)
- 热门话题分析结果会保存为 `trending_topics_时间戳.json`

如果选择了下载媒体文件，图片和视频将保存在 `media` 目录下：
- 目录结构为：`media/关键词/image` 或 `media/关键词/video`
- 文件命名格式：`微博ID_时间戳_随机数.jpg` 或 `微博ID_时间戳_随机数.mp4`

## 关键词管理

运行关键词管理工具：

```bash
python add_keywords.py
```

此工具提供以下功能：
- 添加单个关键词
- 批量添加关键词
- 从文件导入关键词
- 删除关键词
- 导出关键词到Excel
- 查看当前关键词列表

## 媒体文件下载

程序支持下载微博中的图片和视频：

1. 运行爬虫时，选择是否下载媒体文件
2. 选择"y"后，程序会自动下载发现的图片和视频
3. 媒体文件按关键词和类型分别存储在media目录下
4. CSV结果文件中会记录：
   - 图片/视频URL
   - 本地保存路径
   - 是否包含图片/视频的标记

## 机器学习分析

程序提供了机器学习分析功能，帮助筛选有价值的内容：

1. **内容质量评分**：自动评估每条微博的内容质量，计算综合分数
   - 考虑内容长度、互动数据（转发、评论、点赞）
   - 考虑内容情感倾向和关键词权重
   - 考虑是否包含图片和视频

2. **过滤低质量内容**：根据质量分数筛选高质量微博，过滤噪声

3. **热门话题识别**：自动识别数据中的热门话题
   - 提取关键词并计算热度分数
   - 分析关键词的互动指标和关联微博
   - 生成排序后的热门话题列表

4. **话题聚类**：对内容进行主题聚类分析
   - 使用TF-IDF向量化和K-means聚类
   - 识别相似内容的主题组
   - 提取每个聚类的代表性关键词

5. **持续学习**：支持根据用户反馈不断优化模型

### 使用的技术

- **BERT**：用于上下文理解和情感分析
- **XGBoost**：用于内容质量预测评分
- **TF-IDF和K-means**：用于话题聚类
- **jieba分词**：用于中文文本分析和关键词提取

## 注意事项

1. 微博搜索页面可能需要登录才能查看完整内容，建议提供Cookie以提高爬取成功率
2. 爬虫设置了随机延迟，以避免频繁请求被封禁
3. 请合理使用，不要进行大规模爬取，以免对网站造成压力
4. 爬取的内容仅用于个人研究和学习
5. 视频下载可能需要登录状态的Cookie，否则可能无法获取正确的视频URL
6. 图片和视频的下载会增加爬取时间，请耐心等待
7. 首次使用机器学习分析功能会下载模型，可能需要较长时间，请耐心等待

## 文件说明

- `main.py`: 主程序，运行爬虫
- `fetch.py`: 爬虫核心逻辑
- `keyword_manager.py`: 关键词管理工具类
- `add_keywords.py`: 关键词管理的交互式界面
- `ml_analyzer.py`: 机器学习分析模块
- `keywords.txt`: 默认的关键词文件
- `requirements.txt`: 依赖包列表
- `media/`: 存放下载的图片和视频
- `results/`: 存放爬取结果CSV文件
- `models/`: 存放机器学习模型和相关文件

## 可能的问题和解决方案

1. **无法获取搜索结果**
   - 请检查网络连接
   - 尝试提供有效的Cookie
   - 检查关键词是否正确

2. **爬取速度过慢**
   - 这是正常现象，为了避免被封，程序设置了随机延迟
   - 可以考虑减少爬取页数
   - 如果不需要媒体文件，选择不下载可以提高速度

3. **图片或视频无法下载**
   - 可能是因为需要登录状态才能获取媒体文件URL
   - 尝试提供有效的Cookie
   - 检查网络连接状态
   - 查看微博是否被设置了权限限制

4. **机器学习分析错误**
   - 检查是否已安装所有必要的依赖
   - 首次运行时，确保网络连接正常以便下载模型
   - 如果BERT模型初始化失败，程序会自动使用备用方法继续分析 